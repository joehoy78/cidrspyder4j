package me.joehoy.pentest.modules;

import java.io.IOException;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import me.joehoy.pentest.data.Host;

public class Scrape {
	
	public static boolean checkText(String urlString) throws MalformedURLException {
		URL url = new URL(urlString);
		try {
			Map <String, List<String>> headers = UrlUtils.doHeadRequest(url);
			if (((String) headers.get("content-type").get(0)).startsWith("text")) {
				return true;
			} else {
				return false;
			}
		} catch (IOException e) {
			return false;
		}
	}
	
	public static List<String> scrapeLinks(String urlString) throws IOException {
		List<String> output = new ArrayList<String>();
		URL url = new URL(urlString);
		String responseBodyString = UrlUtils.doGetRequest(url);
		Document soup = Jsoup.parse(responseBodyString);
		Elements links = soup.select("a");
		List<String> hrefs = links.eachAttr("href");
		for (String href : hrefs) {
			if (href.startsWith("/")) {
				String relativePath = href.substring(1, href.length());
				URL constructedLink = new URL(url, relativePath);
				output.add(constructedLink.toExternalForm());
			} else {
				output.add(href);
			}
		}
		return output;
	}
	
	public static List<String> crawlHost(String host, List<String> urlStrings, int maxDepth, String userAgent, int delay) throws IOException, InterruptedException {
		Map <String, List<String>> crawlUrls = new HashMap <String, List<String>>();
		URL url;
		int currentCrawlDepth = 0;
		int resultsFound = -1;
		crawlUrls.put("allUrls", new ArrayList<String>());
		crawlUrls.put("alreadyScraped", new ArrayList<String>());
		
		for (String urlString : urlStrings) {
			url = new URL(urlString);
			if (url.getHost().equals(host)) {
				crawlUrls.get("allUrls").add(urlString);
			}
		}
		
		while (currentCrawlDepth < maxDepth && resultsFound != 0) {
			resultsFound = 0;
			for (String urlString : crawlUrls.get("allUrls")) {
				if ((!crawlUrls.get("alreadyScraped").contains(urlString)) && checkText(urlString)) {
					System.out.println("[+] " + urlString);
					List<String> results = scrapeLinks(urlString);
					TimeUnit.SECONDS.sleep(delay/1000);
					int oldSize = crawlUrls.get("allUrls").size();
					for (String result : results) {
						url = new URL(result);
						if (url.getHost().equals(host)) {
							crawlUrls.get("allUrls").add(result);
						}
						List<String> tempUrlList = new ArrayList<String>();
						tempUrlList = Host.sanitise(crawlUrls.get("allUrls"));
						crawlUrls.get("allUrls").clear();
						crawlUrls.get("allUrls").addAll(tempUrlList);
						int newSize = crawlUrls.get("allUrls").size();
						resultsFound += newSize - oldSize;
						crawlUrls.get("alreadyScraped").add(urlString);
					}
				currentCrawlDepth +=1;	
				}
			}
		}
		return crawlUrls.get("alreadyScraped");
	}

}
